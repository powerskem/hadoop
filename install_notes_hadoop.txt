#!/bin/less
#
# install_notes_hadoop.txt
#
# KP Chase
#
# For installing and configuring and running Hadoop on CentOS-7
################################################################


  ############################################################################################
  # Create the first VM with CentOS-7 minimal install.
  # This VM will be cloned to create other nodes after Hadoop is installed.
  ############################################################################################

  # disable IPv6 -- add to the file /etc/sysctl.conf
    net.ipv6.conf.all.disable_ipv6 = 1
    net.ipv6.conf.default.disable_ipv6 = 1

  #################
  # Install needed utilities (as root)
  #################

    su -
    yum install update-java-alternatives
    yum install git
    yum install vim
    update-alternatives --config vi

  # Install plain text browser in order to check web page
    yum -y install lynx

  #OPTIONAL
    yum install wget
    yum install tree
    yum install expect

  #TODO install sw builders
  # yum install ant make
    yum -y install maven
  #TODO install compilers
  # yum install gcc gcc-C++
  #TODO install lib for communication between nodes
  # yum install openssl
  #TODO install lib for development
  # yum install openssl
  #TODO install lib for sql and ldap
  # yum install libxml2-devel libxsit-devel
  # yum install libsqlite ldap
  # check hadoop/java compatability charts \ 
    # @ https://wiki.apache.org/hadoop/HadoopJavaVersions
  # Check current installed java version
    java -version
  # Install desired JDK version
    yum -y install java-1.7.0-openjdk-devel.x86_64 
  # verify alternatives settings link to correct version
    alternatives --display java
  # make sure the bin directory contains the binaries
    ls /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.121-0.b13.el7_3.x86_64/
  # verify /usr/lib/java/ is an empty directory
    ls /usr/lib/java
  # verify /etc/alternatives/java_sdk points to correct version
  # e.g.,   /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.131-2.6.9.0.el7_3.x86_64
    ll /etc/alternatives/java_sdk
  # replace /usr/lib/java with a soft link
    rmdir /usr/lib/java
    ln -s /etc/alternatives/java_sdk/ /usr/lib/java
  # set home path for user
    vi /etc/bashrc ; # add these lines to end of file:
        # set java home dir 
        export JAVA_HOME="/usr/lib/java"
        export PATH="$PATH:$JAVA_HOME/bin"

        # add location for PIDs to /usr/lib/tmpfiles.d/hadoop.conf
            d /var/run/hadoop 0755 hduser hadoop

        # source the Hadoop runtime
        if [ -f ./.hadooprc ]; then
            . ./.hadooprc
        fi

  # change hostname (include in instructions for add'l nodes)
    hostnamectl set-hostname hdcentos
    hostname
  # change hosts file
    vi /etc/hosts ;     # add line:
        $namenode_IP    hdcentos
        $node2_IP       node2
        $node3_IP       node3
        $node4_IP       node4
        $node5_IP       node5
  # reboot
    reboot
    hostname ; # verify new hostname

  # create Hadoop main user
    adduser hduser
    passwd hduser

  #NOT RECOMMENDED
  # add user to sudoers
  # visudo # add line: hduser  ALL=(ALL)    ALL
  # su hduser ; sudo ls ; exit ; # test it

  ####################################
  # yum installs code into /usr/bin and then creates other directories
  ####################################
  ####################################
  # File system layout best practice:
  #   Code base:      /usr/lib/<component>
  #   Config files:   /etc/<component>/conf
  #   Data files:     /var/db/<component>
  #   Log files:      /var/log/<component>
  #   Pid files:      /var/run/<component>
  #   Working dir:    /home/hduser/<component>
  ####################################

  #################
  # Install Hadoop (as root)
  #################

    su -
    ls /etc/yum.repos.d/
  # Use HortonWorks HDP Repo
    curl -O public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.3.0.0/hdp.repo
    mv hdp.repo /etc/yum.repos.d/.
    ll /etc/yum.repos.d/hdp.repo 
    yum -y install epel-release
    yum -y repolist enable
  # search for HDP stack repos
    yum search hadoop
    yum search hive

  # Install hadoop as root
    yum -y install hadoop hadoop-client
    yum -y install hadoop-hdfs hadoop-libhdfs; # HDFS
    yum -y install hadoop-yarn; # YARN
    yum -y install hadoop-mapreduce; # MAPR
    
  #TODO "install the compression libraries, snappy and LZL" <-- no clue what these are for
  # yum -y install snappy snappy-devel

  ####################################
  # Operational dir location (alternative to /usr/local/opt or /opt):
  #   Soft links:     /usr/hdeco
  ####################################

  # Create op dir for soft links
    mkdir /usr/hdeco
    which hadoop            # /usr/bin/hadoop
    which hadoop | ls -l    # hadoop -> /usr/hdp/current/hadoop-client

  # Verify locations
    ls -l /usr/hdp/current

  # Create "swing links" for operational code
    cd /usr/hdeco
    ln -s /usr/hdp/current/hadoop-client hadoop
    ln -s /usr/hdp/current/hadoop-hdfs-client hadoop-hdfs ; # HDFS
    ln -s /usr/hdp/current/hadoop-yarn-client hadoop-yarn ; # YARN
    ln -s /usr/hdp/current/hadoop-mapreduce-client hadoop-mapreduce ; # MAPR

    ls -l   # hadoop -> /usr/hdp/current/hadoop-client
            # hadoop-hdfs -> /usr/hdp/current/hadoop-hdfs-client ; # HDFS
            # hadoop-mapreduce -> /usr/hdp/current/hadoop-mapreduce-client ; # MAPR
            # hadoop-yarn -> /usr/hdp/current/hadoop-yarn-client ; # YARN

  ####################################
  # verify file locations
  ####################################
  # jar files are in /usr/hdp/current/hadoop-client -> /usr/hdp/2.3.0.0-2557/hadoop
    ls -l /usr/hdeco/hadoop
  # hadoop operations files (hadoop, hdfs, yarn, mapred) are in:
    ls -l /usr/hdeco/hadoop/bin
  # sysadmin scripts (hadoop-daemon.sh, slaves.sh) are in:
    ls -l /usr/hdeco/hadoop/sbin
  # hadoop executable is /usr/bin/hadoop -> /usr/hdp/current/hadoop-client/bin/hadoop
    ls -l /usr/bin/hadoop
  # hdfs executable is /usr/bin/hdfs -> /usr/hdp/current/hadoop-hdfs-client/bin/hdfs
    ls -l /usr/bin/hdfs
  ####################################

  # Create libexec links from hadoop-hdfs and hadoop-yarn dirs
  # to point back to /usr/hdeco/hadoop/libexec dir
  # (Newer builds may not require this)
    cd /usr/hdeco/hadoop-hdfs
    ln -s /usr/hdeco/hadoop/libexec libexec; # HDFS
    cd /usr/hdeco/hadoop-yarn
    ln -s /usr/hdeco/hadoop/libexec libexec; # YARN
    cd /usr/hdeco/hadoop-mapreduce
    ln -s /usr/hdeco/hadoop/libexec libexec; # MAPR

  # chown the conf dir and its files (from root:root) to allow r/w by hduser
    ls -l /etc/hadoop ;     # contains conf, a directory (not a link)
    ls /etc/hadoop/conf ;   # contains *-env.sh and *-site.xml files
    chown -R hduser:hadoop /etc/hadoop/conf
    ls -ld /etc/hadoop/conf |grep -e "hduser *hadoop" ; # dir is owned by hduser hadoop
    ls -l /etc/hadoop/conf |grep -e "hduser *hadoop" ;  # all files are owned by hduser hadoop

  # make db dir
    mkdir /var/db/hdfs; # HDFS
    chown -R hduser:hadoop /var/db/hdfs; # HDFS
    ls -l /var/db/hdfs

  # Make sure log files are writable by hduser - They are set in the hadoop-env.sh file
    chown -R hduser:hadoop /var/log/hadoop/hdfs; # HDFS
    chown -R hduser:hadoop /var/log/hadoop/yarn; # YARN
    chown -R hduser:hadoop /var/log/hadoop/mapreduce; # MAPR

  #TODO Make this a permanent change
    mkdir -p  -m0755 /var/run/hadoop
    chown -R hduser:hadoop /var/run/hadoop

  #################
  # Make working dir /home/hduser/hadoop (as hduser)
  #################

  # make working dir in home dir to make it easier to admin and operate hadoop
    su hduser
    cd /home/hduser
    mkdir hadoop
    cd hadoop
    ln -s /etc/hadoop/conf conf ; # testHadoopConfDir
  # verify we can write to this dir
    touch /home/hduser/hadoop/conf/test
    ls /home/hduser/hadoop/conf/ |grep test
    rm /home/hduser/hadoop/conf/test
  # these files are not needed
  # rm *.cmd *.bat

  # link to db dir in home dir
    cd /home/hduser/hadoop
    ln -s /var/db/hdfs hdfs; # HDFS
    cd hdfs
    mkdir namenode datanode checkpoint; # HDFS
    ls -l /home/hduser/hadoop/hdfs

  # link to log dir in home dir
    cd /home/hduser/hadoop
    mkdir log
    mkdir run
    ln -s /var/log/hadoop/hdfs log/hdfs; # HDFS
    ln -s /var/log/hadoop/yarn log/yarn; # YARN
    ln -s /var/log/hadoop/mapreduce log/mapreduce; # MAPR
    ls -l /home/hduser/hadoop/log/*

  # link to run dir in home dir
    ln -s /var/run/hadoop/hdfs run/hdfs; # HDFS
    ln -s /var/run/hadoop/yarn run/yarn; # YARN
    ln -s /var/run/hadoop/mapreduce run/mapreduce; # MAPR
    ls -l /home/hduser/hadoop/run/*

  # make input dir
    cd /home/hduser
    mkdir input

  # create ssh key
    ssh-keygen -t rsa -P ''

  # set up passwd-less login for hduser to hostname hdcentos
    cd ~
    ls .ssh ;       # verify file authorized_keys doesn't already exist
    cp .ssh/id_rsa.pub .ssh/authorized_keys ; # make authorized key list
  # add to list of known hosts
    ssh localhost # answer "yes" and then exit
    ssh localhost # make sure it connects without asking for a passwd
    ssh hdcentos  # answer "yes" and then exit
    ssh hdcentos  # make sure it connects without asking for a passwd

  # make .hadooprc file
    cd /home/hduser
    vi .hadooprc ; # add these lines:
        # .hadooprc
        # set opt install dir
        export OPT_DIR="/usr/hdeco"
        # set hadoop home dirs
        export HADOOP_COMMON_HOME="$OPT_DIR/hadoop"
        export HADOOP_HDFS_HOME="$OPT_DIR/hadoop-hdfs"; # HDFS
        export HADOOP_YARN_HOME="$OPT_DIR/hadoop-yarn"; # YARN
        export HADOOP_MAPRED_HOME="$OPT_DIR/hadoop-mapreduce"; # MAPR
        #TODO set hadoop config dir
        export HADOOP_CONF_DIR=/etc/hadoop/conf
        # set hadoop paths
        export PATH="$PATH:$HADOOP_COMMON_HOME/sbin"
        export PATH="$PATH:$HADOOP_HDFS_HOME/sbin"; # HDFS
        export PATH="$PATH:$HADOOP_YARN_HOME/sbin"; # YARN
        export PATH="$PATH:$HADOOP_MAPRED_HOME/sbin"; # MAPR

    source .bashrc
    echo $PATH | grep "/usr/hdeco/hadoop/sbin"

  # test installation
    hadoop version
    hadoop

  ####################################
  # Edit config files for HDFS
  ####################################

    mydate="$(date +%Y-%m-%d)"
    cd /home/hduser/hadoop/conf
    cp hadoop-env.sh hadoop-env.sh.orig.$mydate
    vi hadoop-env.sh ;  # find LOG_DIR, add line: 
                        export HADOOP_LOG_DIR="/var/log/hadoop/hdfs"; # HDFS
                        # find PID_DIR, add line:
                        export HADOOP_PID_DIR="/var/run/hadoop/hdfs"; # HDFS
    cp core-site.xml core-site.xml.orig.$mydate
    vi core-site.xml ;  # add a property:
                        <property>
                            <name>fs.default.name</name>
                            <value>hdfs://hdcentos:9000</value>
                        </property>
    if [ -f hdfs-site.xml ]; then cp hdfs-site.xml hdfs-site.xml.orig.$mydate ; fi
    cp /usr/hdeco/hadoop/../etc/hadoop/conf.empty/hdfs-site.xml .
    vi hdfs-site.xml ; # add site-specific properties:
                        <property>
                            <name>dfs.replication</name>
                            <value>1</value>
                        </property>
                        <property>
                            <name>dfs.namenode.name.dir</name>
                            <value>file:/var/db/hdfs/namenode</value>
                        </property>
                        <property>
                            <name>dfs.datanode.name.dir</name>
                            <value>file:/var/db/hdfs/datanode</value>
                        </property>

  ####################################
  # TEST CONFIG for HDFS
  ####################################

    hdfs namenode -format
    ls ~/hadoop/hdfs/namenode/current

  ####################################
  # START HDFS
  #TODO move this to a separate shell script
  ####################################

  # start the daemons for pseudo-distrib mode IN THIS ORDER
    hadoop-daemon.sh start namenode
    hadoop-daemon.sh start datanode
    hadoop-daemon.sh start secondarynamenode

  # List java processes
    jps ; # should be all three showing

  # Use hdfs dfs to ls root dir
    hdfs dfs -ls / ; # should give no output

  # find errors
    ls /home/hduser/hadoop/log/hdfs
    grep ERR /home/hduser/hadoop/log/hdfs/*

  #TODO fwd port from host to vm
  # check web server for hdfs -- Navigate in browser to http://hdcentos:50070

  # shut down daemons IN THIS ORDER  
    hadoop-daemon.sh stop secondarynamenode
    hadoop-daemon.sh stop datanode
    hadoop-daemon.sh stop namenode

  # List java processes
    jps ; # should be none showing

  ####################################
  # Edit config files for YARN
  ####################################

    mydate="$(date +%Y-%m-%d)"
    cd /home/hduser/hadoop/conf
    if [ -f yarn-env.sh ]; then cp yarn-env.sh yarn-env.sh.orig.$mydate ; fi
    cp /usr/hdeco/hadoop/../etc/hadoop/conf.empty/yarn-env.sh .
    vi yarn-env.sh ;  # find YARN_LOG_DIR, change line: 
                        export YARN_LOG_DIR="/var/log/hadoop/yarn"; # YARN
                        # add line:
                        export YARN_PID_DIR="/var/run/hadoop/yarn"; # YARN
    if [ -f yarn-site.xml ]; then cp yarn-site.xml yarn-site.xml.orig.$mydate ; fi
    cp /usr/hdeco/hadoop/../etc/hadoop/conf.empty/yarn-site.xml .
    vi yarn-site.xml ; # add site-specific properties:
                        <property>
                            <name>yarn.resourcemanager.address</name>
                            <value>hdcentos:8032</value>
                        </property>
                        <property>
                            <name>yarn.resourcemanager.scheduler.address</name>
                            <value>hdcentos:8030</value>
                        </property>
                        <property>
                            <name>yarn.resourcemanager.tracker.address</name>
                            <value>hdcentos:8031</value>
                        </property>
                        <property>
                            <name>yarn.nodemanager.aux-service</name>
                            <value>mapreduce_shuffle</value>
                        </property>
                        <property>
                            <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
                            <value>org.apache.hadoop.mapred.ShuffleHandler</value>
                        </property>
                        <property>
                            <name>yarn.application.classpath</name>
                            <value>.,/usr/hdeco/hadoop/*,/usr/hdeco/hadoop/lib/*,/usr/hdeco/hadoop-hdfs/*,/usr/hdeco/hadoop-hdfs/lib/*,/usr/hdeco/hadoop-yarn/*,/usr/hdeco/hadoop-yarn/lib/*,/usr/hdeco/hadoop-mapreduce/*,/usr/hdeco/hadoop-mapreduce/lib/*</value>
                        </property>

  ####################################
  # START YARN
  ####################################

  # start the daemon
    hadoop-daemon.sh start namenode
    hadoop-daemon.sh start datanode
    hadoop-daemon.sh start secondarynamenode
    yarn-daemon.sh start resourcemanager
    yarn-daemon.sh start nodemanager

  # List java processes
    jps ; # should show 2 new processes

  # find errors
    ls /home/hduser/hadoop/log/yarn
    grep ERR /home/hduser/hadoop/log/yarn/*

  # shut down daemons IN THIS ORDER  
    yarn-daemon.sh stop nodemanager
    yarn-daemon.sh stop resourcemanager
    hadoop-daemon.sh stop secondarynamenode
    hadoop-daemon.sh stop datanode
    hadoop-daemon.sh stop namenode

  # List java processes
    jps ; # should be none showing

  #TODO fwd port from host to vm
  # check web server for hdfs -- Navigate in browser to http://hdcentos:8088

  ####################################
  # Edit config files for MAPR
  ####################################

    mydate="$(date +%Y-%m-%d)"
    cd /home/hduser/hadoop/conf
    if [ -f mapred-env.sh ]; then cp mapred-env.sh mapred-env.sh.orig.$mydate ; fi
    vi mapred-env.sh ;  # find HADOOP_MAPRED_LOG_DIR, change line: 
                        export HADOOP_MAPRED_LOG_DIR="/var/log/hadoop/mapreduce" ; # MAPR
                        # add line:
                        export HADOOP_MAPRED_PID_DIR="/var/run/hadoop/mapreduce" ; # MAPR
    if [ -f mapred-site.xml ]; then cp mapred-site.xml mapred-site.xml.orig.$mydate ; fi
    cp /usr/hdeco/hadoop/../etc/hadoop/conf.empty/mapred-site.xml .
    vi mapred-site.xml ; # add site-specific properties:
                        <property>
                            <name>mapreduce.framework.name</name>
                            <value>yarn</value>
                        </property>
                        <property>
                            <name>mapreduce.jobhistory.address</name>
                            <value>hdcentos:10020</value>
                        </property>

  ####################################
  # START MAPR
  #TODO move this to a separate shell script
  ####################################

  # start the daemon
    hadoop-daemon.sh start namenode
    hadoop-daemon.sh start datanode
    hadoop-daemon.sh start secondarynamenode
    yarn-daemon.sh start resourcemanager
    yarn-daemon.sh start nodemanager
    mr-jobhistory-daemon.sh start historyserver

  # List java processes
    jps

  # Check status
    hdfs dfsadmin -report

  # shut down daemons IN THIS ORDER
    mr-jobhistory-daemon.sh stop historyserver
    yarn-daemon.sh stop nodemanager
    yarn-daemon.sh stop resourcemanager
    hadoop-daemon.sh stop secondarynamenode
    hadoop-daemon.sh stop datanode
    hadoop-daemon.sh stop namenode

  ####################################
  # Set up data dirs
  ####################################
    hdfs ; # gives commands
  # make a dir for data (if not already done)
  # mkdir /home/hduser/data
    hdfs dfs -ls
    hdfs dfs -mkdir -p /user/hduser/
    hdfs dfs -ls
    hdfs dfs -put data/books/Civil.txt /user/hduser/
    hdfs dfs -ls ; # verify it's there
    hdfs dfs -tail Civil.txt ; # verify its contents

  ############################################################################################
  # Test the installation
  ############################################################################################

  ####################################
  # On fresh startup...
  ####################################
  # Source /home/hduser/.bashrc
    su hduser
    cd /home/hduser
    source .bashrc
 
  ####################################
  # RUN HADOOP IN Pseudo-Distributed Mode
    (on a single-node where each Hadoop daemon runs in a separate Java process)
  ####################################
  # start the daemon
    hadoop-daemon.sh start namenode
    hadoop-daemon.sh start datanode
    hadoop-daemon.sh start secondarynamenode
    yarn-daemon.sh start resourcemanager
    yarn-daemon.sh start nodemanager
    mr-jobhistory-daemon.sh start historyserver

  # List java processes
    jps #testProcessesStart

  # Check status
    hdfs dfsadmin -report

  # check web server for namenode -- Navigate in browser to http://hdcentos:50070
  # check web server for resourcemanager -- Navigate in browser to http://hdcentos:8088

  ####################################
  # Run hadoop job #1
  ####################################
    hdfs dfs -rm out/*
    hdfs dfs -rmdir out
    hdfs dfs -ls
    hadoop jar $HADOOP_MAPRED_HOME/hadoop-mapreduce-examples.jar wordcount Civil.txt out
    hdfs dfs -ls out
    hdfs dfs -cat out/part-r-00000 |sort -k2 -n | tail

    hdfs dfs -rm out/*
    hdfs dfs -rmdir out
    hadoop jar $HADOOP_MAPRED_HOME/hadoop-mapreduce-examples.jar grep Civil.txt out 'my[a-z.]+'
    hdfs dfs -cat out/part-r-00000 | head

  ####################################
  # Run hadoop job #2
  ####################################
    hadoop jar $HADOOP_MAPRED_HOME/hadoop-mapreduce-examples.jar pi 2 2

  ####################################
  # Check for errors
  ####################################
    mydate="$(date +%Y-%m-%d)"
    mydatewhr="$(date +%Y-%m-%d\ %H)"
    grep -r ERR /home/hduser/hadoop/log/* |grep $mydate
    find -L /home/hduser/hadoop/log -mtime -1 | xargs /bin/ls -atd1Lp
    find -L /home/hduser/hadoop/log -mmin -5 | xargs /bin/ls -atd1Lp
    find -L /home/hduser/hadoop/log -mmin -5 | xargs /bin/grep -s ERR -c
    find -L /home/hduser/hadoop/log -mmin -5 | xargs /bin/grep -s ERR -l
    find -L /home/hduser/hadoop/log -mmin -5 | xargs /bin/grep -s ERR -l | xargs /bin/grep -s ERR | grep "$mydatewhr"

    lynx http://hdcentos:50070
    lynx http://hdcentos:8088














  ############################################################################################
  # Older instructions...
  ############################################################################################

    # propagate to slave nodes
    cd /opt
    for x in 1 2 3 4; do scp -r hadoop hslave${x}:/opt ; done

# create HDFS DataNode data dir
    ./nodes_cmd.exp root "mkdir /home/hadoop/datanode"
    ./nodes_cmd.exp root "chown hadoop /home/hadoop/datanode -R"
    ./nodes_cmd.exp root "chgrp hadoop /home/hadoop/datanode -R"

# ON MASTER ONLY create HDFS NameNode data dir
    mkdir /home/hadoop/namenode
    chown hadoop /home/hadoop/namenode
    chgrp hadoop /home/hadoop/namenode

# ON MASTER ONLY edit the file /opt/hadoop/etc/hadoop/hdfs-site.xml
    addition="<configuration>\n<property>\n<name>dfs.namenode.data.dir<\/name>\n<value>\/home\/hadoop\/namenode<\/value>\n<\/property>"
    sedscript="s/<configuration>/$addition/"
    sed -e $sedscript -i /opt/hadoop/etc/hadoop/hdfs-site.xml

# ON MASTER ONLY edit the file /opt/hadoop/etc/hadoop/mapred-site.xml
    addition="<configuration>\n<property>\n<name>mapreduce.framework.name<\/name>\n<value>yarn<\/value>\n<\/property>"
    sedscript="s/<configuration>/$addition/"
    cp /opt/hadoop/etc/hadoop/mapred-site.xml.template /opt/hadoop/etc/hadoop/mapred-site.xml
    sed -e $sedscript -i /opt/hadoop/etc/hadoop/mapred-site.xml

# ON MASTER ONLY create the file /opt/hadoop/etc/hadoop/slaves
#TODO Make note that start-dfs.sh looks at "workers" instead of "slaves" file
    hmaster
    hslave1
    hslave2
    hslave3
    hslave4

# ON MASTER ONLY create the file $HADOOP_YARN_HOME/conf/masters
    mkdir /opt/hadoop/conf
    touch /opt/hadoop/conf/masters
    echo hmaster >> /opt/hadoop/conf/masters

    chown hadoop /opt/hadoop/conf -R
    chgrp hadoop /opt/hadoop/conf -R

# add to the file /opt/hadoop/etc/hadoop/mapred-site.xml
    addition="<configuration>\n<property>\n<name>mapred.job.tracker<\/name>\n<value>hmaster:9000<\/value>\n<\/property>"
    sedscript="s/<configuration>/$addition/"
    cp /opt/hadoop/etc/hadoop/mapred-site.xml.template /opt/hadoop/etc/hadoop/mapred-site.xml
    sed -e $sedscript -i /opt/hadoop/etc/hadoop/mapred-site.xml

# add to file yarn-site.xml to fix error:
#   "Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster"
    <property>
        <name>yarn.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*</value>
    </property>

# add to file mapred-site.xml to fix error:
#   "Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster"
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*</value>
    </property>

# open ports with either iptables or firewall
#   ssh root@hmaster
#     iptables -I INPUT -p tcp --dport 9000 -j ACCEPT
#     iptables -I INPUT -p tcp --dport 9001 -j ACCEPT
#     for x in 1 2 3 4; do ssh hslave{x} "iptables -I INPUT -p tcp --dport 50010 -j ACCEPT" ; done
#
#   #ssh ./nodes_cmd.exp root "systemctl iptables save ; service iptables reload"
#   ./nodes_cmd.exp root "systemctl stop firewalld"
#
# OR...
    ssh root@hmaster 
      firewall-cmd --permanent --add-port=9000/tcp
      firewall-cmd --permanent --add-port=9001/tcp
      firewall-cmd --reload
      for x in 1 2 3 4; do ssh hslave${x} "firewall-cmd --permanent --add-port=50010/tcp ; firewall-cmd --reload" ; done

##################################################################
# disable IPv6 -- add to the file /etc/sysctl.conf
    net.ipv6.conf.all.disable_ipv6 = 1
    net.ipv6.conf.default.disable_ipv6 = 1
##################################################################

# edit the file etc/hadoop/hadoop-env.sh
    javahm=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
    ./nodes_cmd.exp root "echo -e \"export JAVA_HOME=$javahm\" >> /opt/hadoop/etc/hadoop/hadoop-env.sh"

# add to the file /etc/bashrc
    export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
    export JRE_HOME=$JAVA_HOME/jre
    export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
    systemctl stop firewalld

# ON MASTER ONLY format namenode
    ssh hadoop@hmaster 
    hdfs namenode -format
    exit
    
# start hadoop cluster
    ssh hadoop@hmaster "start-dfs.sh"

############# troubleshooting ############## 
# check cluster
    hdfs dfsadmin -report

# stop the cluster
    stop-all.sh
    mapred --daemon stop historyserver
    for x in 1 2 3 4; do ssh hslave${x} "stop-all.sh" ; done

# empty the datanode and namenode dirs on hmaster and slaves
    rm -vrf /home/hadoop/datanode/* ; rm -vrf /home/hadoop/namenode/* ; rm -vrf /tmp/hadoop-*hadoop/*
    for x in 1 2 3 4; do ssh hslave${x} "rm -vrf /home/hadoop/datanode/* ; rm -vrf /tmp/hadoop-*hadoop/*" ; done

# format namenode on hmaster
    hdfs namenode -format

# view logs
    # ./nodes_cmd.exp root "tail /opt/hadoop/logs/*"

############################################ 
# restart hadoop cluster
    start-dfs.sh
    for x in 1 2 3 4; do ssh hslave${x} "hdfs --daemon start datanode" ; done

# start yarn
    start-yarn.sh

# start job history
    #$HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver
    #mr-jobhistory-daemon.sh start historyserver
    mapred --daemon start historyserver

# test
    jps

# check cluster
    hdfs dfsadmin -report

############################################ 
#create dir
    hadoop fs -mkdir /input

#store data to hadoop fs
    hadoop fs -copyFromLocal ./test.txt /input

#ls dir
    hadoop fs -ls /input

#view data
    hadoop fs -cat /input/test.txt

############################################ 
# install Apache on hmaster
    yum install httpd


